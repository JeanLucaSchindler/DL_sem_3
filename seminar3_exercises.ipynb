{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY475 Seminar 3: Convolutional neural networks and autoencoders\n",
    "\n",
    "In this seminar, we are going to study working with image data using both *convolutional neural networks* (CNNs) and *(variational) autoencoders*.\n",
    "\n",
    "As normal, we will build our networks in three stages:\n",
    "\n",
    "1. Format the input data\n",
    "2. Design the neural network architecture\n",
    "3. Train the model\n",
    "\n",
    "Our first dataset is the cannonical [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, a set of handwritten digits with the corresponding numerical label from 0-9. The first task is to recognise which digit each handwritten example represents, making this a *multi-class classification problem* as there are 10 outcome classes (0, 1,...,9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0. Reading in images\n",
    "\n",
    "Fortunately, the MNIST data is so common it comes bundled with `torchvision` (usually this module gets installed at the same time as the base pytorch), so we can use inbuilt functions to download the data. Before we get to the full data, however, it's worth just understanding how these images get converted into numerical vectors (also useful for exercise 2 below).\n",
    "\n",
    "Complete the below code to read in a single image, and display it using the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "example = Image.open(\"mnist_example.jpg\")\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(example, cmap=____)\n",
    "plt.show()\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "pixel_array = np.array(example)\n",
    "pixel_array[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Building a convolutional neural network\n",
    "\n",
    "### Data formatting\n",
    "\n",
    "We'll download our data using a specific function in the `torchvision.datasets` submodule. This function will download the data into your working directory. We can also specify whether we want the cannonical `train` or `test` splits of the data, and since we want to represent these as tensors, we can use the `torchvision.transforms` submodule function `ToTensor()` to convert the data into the correct format.\n",
    "\n",
    "Complete the code below to generate both a train and a test dataset, as well as corresponding dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test sets for the MNIST data using the torch package\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"./mnist_data\", download=True, train=True, transform=ToTensor()\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Get the corresponding test data\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"./mnist_data\", download=True, train=False, transform=ToTensor()\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the CNN architecture\n",
    "\n",
    "Now we can design our CNN to classify these images.\n",
    "\n",
    "The key, new bits of code we will need to use are:\n",
    "\n",
    "`torch.nn.Conv2d` - a 2D convolutional layer to create our feature maps\n",
    "\n",
    " * This function takes in a minimum of three arguments: the number of input channels (`in_channels`), the number of output channels (`out_channels`), and the kernel size (you guessed it...`kernel_size`!)\n",
    " * You can also modify the `stride` (default = 1) and amount of `padding` (default = 0). Notice, in the default implementation, as there is no padding there will be some reduction in the image size after each cross-correlation operation. If you want to downsample the image only through pooling, you can set `padding = 'same'` to keep the image size the same after the cross-correlation.\n",
    " * You can review the full documentation for this layer [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "\n",
    "`torch.nn.MaxPool2d` - a 2D pooling layer we can use to downsample our feature maps\n",
    "\n",
    " * For a given `kernel_size` and `stride`, this layer will take the maximum value in each `kernel_size` x `kernel_size` window and output it to the next layer\n",
    " * This operation has the effect of reducing the resolution of our feature maps\n",
    " * Note: as with dropout, this is a functional operation and so we only need to define this attribute once (and then can apply it multiple times in the forward pass)\n",
    " * The full documentation can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "We will then use fully connected layers (as per last week) at the end of our convolutions before making our final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network\n",
    "\n",
    "Your task is to complete the below code to build a simple CNN with the following architecture:\n",
    "\n",
    "1. Two convolutional layers. The first layer should yield 2 feature maps, and the second layer should yield 4 feature maps. You should use a kernel size of 3 for both layers, but prevent these cross-correlations from reducing the dimensionality of the inputs (using the `padding` argument)\n",
    "2. Max pooling layers with a kernel size of 2 and stride of 2 to downsample the resolution of the image\n",
    "3. Two fully connected layers with 64 and 32 nodes respectively\n",
    "4. An output layer with 10 features (one for each digit)\n",
    "\n",
    "You will need to *flatten* your convolutions prior to passing the data to the fully connected layers. Rather than using 'reshape' here, which would duplicate the objects in memory, we can use `torch.view`, which will also change the shape of the tensor but using the existing data (and thus preserving memory).\n",
    "\n",
    "To calculate the number of input features to the first fully connected layer, you should consult the documentation for the `torch.nn.Conv2d` and `torch.nn.MaxPool2d` for the required formulas (or do it from first principles in your head!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = ____\n",
    "        self.conv2 = torch.nn.Conv2d(2, 4, kernel_size=3, padding=1)\n",
    "        self.fc1 = ____\n",
    "        self.fc2 = ____\n",
    "        self.out = torch.nn.Linear(32, 10)\n",
    "\n",
    "        self.hidden_act = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.____(kernel_size=2, stride=____)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return torch.nn.functional.____(self.forward(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your model, we can take the very first training image example and pass it through our untrained model to check the output size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the img to a tensor and test the output of the CNN\n",
    "img_tensor = mnist_train[0][0]\n",
    "cnn = ____()\n",
    "output = cnn(img_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network\n",
    "\n",
    "Finally, we will train the neural network using the MNIST data. We won't worry about a validation dataset today, because the runtimes will be so long we will only loop through our data twice!\n",
    "\n",
    "Complete the code below to train the neural network. You will need to define a loss function and an optimizer, and then loop over the data to train the model.\n",
    "\n",
    "You should use cross entropy loss and the `torch.optim.Adam` optimizer.\n",
    "\n",
    "**Warning!** Always read the PyTorch documentation for the function you are using. In our discussions of multi-class classification, we defined the output activation as a softmax function (converting logits to probabilities for each class). But the cross entropy loss function in PyTorch already includes a softmax operation, so we should not apply the softmax function to the output of our neural network. If you added a softmax activation before, remove this line and rerun your model definition before proceeding.\n",
    "\n",
    "  * If you want to get class probabilities, you could define a new method `predict_proba` that applies the softmax function to the output of your neural network (i.e. by calling `forward()` and then activating the results). This method would not be used during training, but could be useful for making predictions after training is complete. *You do not need to implement this now in order to complete these exercises*\n",
    "\n",
    "Even though the number of feature maps is very small, the sheer quantity of training data means the code will take a while to run (approx. 3 minutes on a M1 Max MacBook Pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "\n",
    "criterion = (\n",
    "    torch.nn.____()\n",
    ")  # notice the loss function requires the output to be unnormalized logits (i.e. not softmax)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the CNN\n",
    "n_epochs = 2\n",
    "cnn.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(trainloader):\n",
    "        optimizer.____()\n",
    "        outputs = cnn(____)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        ____.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Batch #{i}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time\n",
    "\n",
    "Finally, we can assess how well our model performs on out-of-sample data using the test set and classification accuracy metric. Rather than worry too much about converting logits to probabilities, since the maximum logit will have the highest probability, we can just use `torch.max` to find the most likely class. This function returns both the maximum value and the index of the maximum value (we want the latter), and we apply this over the 1st dimension to get the most likely class for each image.\n",
    "\n",
    "A little caveat: PyTorch is fantastic, but for some functions the documentation can be a bit lacking. So while you should always read it, you may also need to test it. Here, the `torch.max` [documentation](https://pytorch.org/docs/2.6/generated/torch.max.html#torch-max) simply states it 'returns the maximum value' but, as you'll see below, it returns *both* that value **and** the corresponding index of that value. So just be mindful of this as you implement novel features from this library.\n",
    "\n",
    "Complete and run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cnn.____()\n",
    "    correct_count = 0\n",
    "    for imgs, labels in testloader:\n",
    "        # get logits for batch\n",
    "        outputs = cnn(____)\n",
    "        # get the predicted class\n",
    "        _, predicted = torch.max(____, 1)\n",
    "        correct_count += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100*correct_count/len(____)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Implement a CNN for the CIFAR-10 dataset\n",
    "\n",
    "A somewhat more challenging dataset is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). Can you modify your previous code to work with this dataset where images are 32x32 pixes and have three channels (RGB)? The dataset can be loaded through Pytorch as well with `datasets.CIFAR10` instead of `datasets.MNIST`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define a transform to normalise the data with mean and sd\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Download and load the CIFAR10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar10_data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Define the classes in CIFAR10\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    # Convert tensor to numpy array and transpose from (C,H,W) to (H,W,C)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # Unnormalize if the data was normalized with mean and std\n",
    "    img = img * np.array([0.5, 0.5, 0.5]) + np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "    # Clip values to be between 0 and 1\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Randomly select 16 images\n",
    "random_indices = random.sample(range(len(trainset)), 16)\n",
    "random_images = [trainset[i][0] for i in random_indices]\n",
    "random_labels = [trainset[i][1] for i in random_indices]\n",
    "\n",
    "# Create a figure with a 4x4 grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Plot each image\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(imshow(random_images[i]))\n",
    "    ax.set_title(f\"{classes[random_labels[i]]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample of CIFAR10 Images\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. MNIST (variational) autoeconder\n",
    "\n",
    "Revisiting the autoencoders from the lecture, can you improve the reconstruction and also the sampling of new MNIST images? Options could e.g. be to increase the number of units in the bottleneck representation, adding convolutional layers, or further fully connected layers. You could also add structure by leveraging the knowledge that MNIST digits are grey-scale values between 0 and 1, and incorporate a sigmoid activation function in the output layer and a binary cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
